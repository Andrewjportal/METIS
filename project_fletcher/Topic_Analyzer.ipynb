{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "from pytube import Playlist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "import bleach\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from gensim.summarization import keywords\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript(url):\n",
    "    path=url\n",
    "    try:\n",
    "        yt = YouTube(path)\n",
    "    except ValueError:\n",
    "        print('cannot find video')\n",
    "    caption = yt.captions.get_by_language_code('en')\n",
    "    try:\n",
    "        xml=caption.xml_captions\n",
    "    except AttributeError:\n",
    "        print('no captions or transcripts')\n",
    "\n",
    "    root = ET.fromstring(xml)\n",
    "    #gets the transcripts\n",
    "    doc=''\n",
    "    for child in root:\n",
    "        try:\n",
    "            doc=doc+\" \"+(child.text)\n",
    "        except TypeError:\n",
    "            pass\n",
    "    return doc.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_corpus(url_list):\n",
    "    corpus=[]\n",
    "    for url in url_list:\n",
    "        x=bleach.clean(get_transcript(url), tags=[], attributes={}, styles=[], strip=True)\n",
    "        y=re.sub(r'&#39;', '', x)\n",
    "        z=re.sub(r'\\[inaudible]', '', y)\n",
    "        doc=re.sub(r'\\[Music]', '', z)\n",
    "       \n",
    "        corpus.append(doc)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oov(keys):\n",
    "    keys2=[]\n",
    "    for key in keys:\n",
    "        if key in model.vocab:\n",
    "            keys2.append(key)\n",
    "    x=len(keys)-len(keys2)\n",
    "    y=x*(sum(list(map(model.word_vec,keys2)))/len(keys2))\n",
    "    vector=sum(list(map(model.word_vec,keys2)))+y\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_space(url_list):\n",
    "    docs=make_corpus(url_list)\n",
    "    vectors_list=[]\n",
    "    for i in range(len(docs)):\n",
    "        keys=keywords(docs[i], words=5,pos_filter=('NN','NNS','NNPS','NNP',),lemmatize=True, split=True)\n",
    "    \n",
    "        try:\n",
    "            vector=sum(list(map(model.word_vec,keys)))\n",
    "        except KeyError:\n",
    "            vector=oov(keys)\n",
    "                \n",
    "        vectors_list.append(vector)\n",
    "   \n",
    "    return (sum(vectors_list)/len(docs))\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_analyze(url):\n",
    "    analysis={}\n",
    "    doc=make_corpus([url])\n",
    "    x=get_topic_space([url])\n",
    "    analysis['Border Wall']=cos_sim(x,wall_vector)\n",
    "    analysis['Stormy Dan']=cos_sim(x,stormy_vector)\n",
    "    analysis['Meuller']=cos_sim(x,mueller_vector)\n",
    "    analysis['NBA']=cos_sim(x,NBA_vector)\n",
    "    analysis['Pokemon']=cos_sim(x,poke_vector)\n",
    "    clean_doc=remove_stopwords(doc[0])\n",
    "    keys=keywords(clean_doc,words=5,pos_filter=('NN','NNS','NNPS','NNP',),lemmatize=True,split=True)\n",
    "   \n",
    "    print (keys)\n",
    "    print(summarize(doc[0],ratio=.01))\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "glove_file = datapath('/Users/andrewportal/Downloads/glove/glove.6B.100d.txt')\n",
    "tmp_file = get_tmpfile(\"glove_word2vec.txt\")\n",
    "\n",
    "# call glove2word2vec script\n",
    "# default way (through CLI): python -m gensim.scripts.glove2word2vec --input <glove_file> --output <w2v_file>\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "wall_keys=['border','wall','immigration','funding','promise']\n",
    "wall_vector=sum(list(map(model.word_vec, wall_keys)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "mueller_keys=['probe','investigation','election', \"collusion\",\"interference\"]\n",
    "mueller_vector=sum((list(map(model.word_vec,mueller_keys))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "stormy_keys=['affair', 'president','lawyer','payment','campaign']\n",
    "stormy_vector=sum((list(map(model.word_vec,stormy_keys))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBA_keys=['conference', 'basketball','league','championship','playoffs']\n",
    "NBA_vector=sum(list(map(model.word_vec, NBA_keys)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "poke_keys=['pokemon','pikachu','nintendo','videogame','fun']\n",
    "poke_vector=sum(list(map(model.word_vec, poke_keys)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['circle', 'theta', 'block', 'masses', 'point']\n",
      "If that first block has a mass which is some power of 100 times the mass of the second, for example 1,000,000 times as much, an insanely surprising fact popped out: The total number of collisions, including those between the second mass and the wall, has the same starting digits as pi.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Border Wall': 0.38666528,\n",
       " 'Stormy Dan': 0.31513035,\n",
       " 'Meuller': 0.32414573,\n",
       " 'NBA': 0.35286573,\n",
       " 'Pokemon': 0.07784927}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_analyze('https://youtu.be/jsYwFizhncE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trump's wall playlist\n",
    "pl=Playlist(\"https://www.youtube.com/playlist?list=PL-nbJMikieaX9joE-O0kj52VP0vqBDlXP\")\n",
    "pl.populate_video_urls()\n",
    "url_list_wall=pl.video_urls\n",
    "#Get trump's wall topic space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "wall_space=get_topic_space(url_list_wall)\n",
    "np.save('wall',wall_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
