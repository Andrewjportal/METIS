{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytube import YouTube\n",
    "from pytube import Playlist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from xml.etree import ElementTree as ET\n",
    "import bleach\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora, models, similarities, matutils\n",
    "from gensim.summarization import keywords\n",
    "from gensim.summarization.summarizer import summarize\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(a,b):\n",
    "    return dot(a, b)/(norm(a)*norm(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript(url):\n",
    "    path=url\n",
    "    try:\n",
    "        yt = YouTube(path)\n",
    "    except ValueError:\n",
    "        print('cannot find video')\n",
    "    caption = yt.captions.get_by_language_code('en')\n",
    "    try:\n",
    "        xml=caption.xml_captions\n",
    "    except AttributeError:\n",
    "        print('no captions or transcripts')\n",
    "\n",
    "    root = ET.fromstring(xml)\n",
    "    #gets the transcripts\n",
    "    doc=''\n",
    "    for child in root:\n",
    "        try:\n",
    "            doc=doc+\" \"+(child.text)\n",
    "        except TypeError:\n",
    "            pass\n",
    "    return doc.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_corpus(url_list):\n",
    "    corpus=[]\n",
    "    for url in url_list:\n",
    "        x=bleach.clean(get_transcript(url), tags=[], attributes={}, styles=[], strip=True)\n",
    "        y=re.sub(r'&#39;', '', x)\n",
    "        z=re.sub(r'\\[inaudible]', '', y)\n",
    "        doc=re.sub(r'\\[Music]', '', z)\n",
    "        corpus.append(doc)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oov(keys):\n",
    "    keys2=[]\n",
    "    for key in keys:\n",
    "        if key in model.vocab:\n",
    "            keys2.append(key)\n",
    "    x=len(keys)-len(keys2)\n",
    "    y=x*(sum(list(map(model.word_vec,keys2)))/len(keys2))\n",
    "    vector=sum(list(map(model.word_vec,keys2)))+y\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_space(url_list):\n",
    "    docs=make_corpus(url_list)\n",
    "    vectors_list=[]\n",
    "    for i in range(len(docs)):\n",
    "        keys=keywords(docs[i], words=10,lemmatize='True', split='true')\n",
    "    \n",
    "        try:\n",
    "            vector=sum(list(map(model.word_vec,keys)))\n",
    "        except KeyError:\n",
    "            vector=oov(keys)\n",
    "                \n",
    "        vectors_list.append(vector)\n",
    "   \n",
    "    return (sum(vectors_list)/len(docs))\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_analyze(url):\n",
    "    analysis={}\n",
    "    doc=make_corpus([url])\n",
    "    x=get_topic_space([url])\n",
    "    analysis['Border Wall']=cos_sim(x,wall_vector)\n",
    "    analysis['Stormy Dan']=cos_sim(x,stormy_vector)\n",
    "    analysis['Meuller']=cos_sim(x,mueller_vector)\n",
    "    analysis['NBA']=cos_sim(x,NBA_vector)\n",
    "    analysis['Pokemon']=cos_sim(x,poke_vector)\n",
    "    keys=keywords(doc[0],words=5,pos_filter=('NN','NNS','NNPS','NNP'),scores='True', lemmatize='True')\n",
    "   \n",
    "    print (keys)\n",
    "    print(summarize(doc[0],word_count=50))\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "glove_file = datapath('/Users/andrewportal/Downloads/glove/glove.6B.100d.txt')\n",
    "tmp_file = get_tmpfile(\"glove_word2vec.txt\")\n",
    "\n",
    "# call glove2word2vec script\n",
    "# default way (through CLI): python -m gensim.scripts.glove2word2vec --input <glove_file> --output <w2v_file>\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(tmp_file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trump's wall playlist\n",
    "pl=Playlist(\"https://www.youtube.com/playlist?list=PL-nbJMikieaX9joE-O0kj52VP0vqBDlXP\")\n",
    "pl.populate_video_urls()\n",
    "url_list_wall=pl.video_urls\n",
    "#Get trump's wall topic space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wall_space=get_topic_space(url_list_wall)\n",
    "np.save('wall',wall_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic=Pokemon\n",
    "pl=Playlist(\"https://www.youtube.com/playlist?list=PLqimesWokesHzcUahlesZ5iBhND1fmWzQ\")\n",
    "pl.populate_video_urls()\n",
    "url_list_poke=pl.video_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get pokemon topic space\n",
    "poke_space=get_topic_space(url_list_poke)\n",
    "np.save('Poke',poke_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Stormy danaiels topic\n",
    "\n",
    "pl=Playlist(\"https://www.youtube.com/watch?v=xukWkOv4a-w&list=PLjBJYbCV8XMyflxLdH4KEg_8OEMCfvvmV\")\n",
    "pl.populate_video_urls()\n",
    "url_list_stormy=pl.video_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stormy_space=get_topic_space(url_list_stormy)\n",
    "np.save('stormy',stormy_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Mueller Topics from a playlist\n",
    "pl=Playlist(\"https://www.youtube.com/watch?v=pgzThHiYOD4&list=PLpgAjMbrNMIKn_IavsRKcmmePoNKMOkXI\")\n",
    "pl.populate_video_urls()\n",
    "url_list_mueller=pl.video_urls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get mueller topics from a list\n",
    "url_list_mueller=['https://www.youtube.com/watch?v=pcq7Fo\\\n",
    "-E56M&index=8&list=PLpgAjMbrNMIKn_IavsRKcmmePoNKMOkXI&t=0s','https://www.youtube.com/watch?v=uCVDEEMZQec','\\\n",
    "https://www.youtube.com/watch?v=1M_CSsQas60']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get topic space\n",
    "meuller_space=get_topic_space(url_list_mueller)\n",
    "np.save('mueller',mueller_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wall_keys=['border','wall','immigration','funding','promise']\n",
    "wall_vector=sum(list(map(model.word_vec, wall_keys)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mueller_keys=['probe','investigation','election', \"collusion\",\"interference\"]\n",
    "mueller_vector=sum(list(map(model.word_vec,mueller_keys)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stormy_keys=['affair', 'president','lawyer','payment','campaign']\n",
    "stormy_vector=sum(list(map(model.word_vec,stormy_keys)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBA_keys=['conference', 'basketball','league','championship','playoffs']\n",
    "NBA_vector=sum(list(map(model.word_vec, NBA_keys)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poke_keys=['pokemon','pikachu','nintendo','videogame','fun']\n",
    "poke_vector=sum(list(map(model.word_vec, poke_keys)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_analyze('https://www.youtube.com/watch?v=BbHLPBJvSOc&t=2701s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=make_corpus(['https://www.youtube.com/watch?v=jsYwFizhncE'])\n",
    "cos=[]\n",
    "for i in range(5,101,5):\n",
    "        \n",
    "    keys=keywords(doc[0],words=i,lemmatize='True', split='true')\n",
    "    try:\n",
    "        x=sum(list(map(model.word_vec,keys)))\n",
    "    except KeyError:\n",
    "        x=oov(keys)\n",
    "    cos.append(cos_sim(x,poke_space))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=cos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=cos\n",
    "x=list(range(5,101,5))\n",
    "sns.set(style=\"ticks\", rc={\"lines.linewidth\": 2})\n",
    "ax=sns.pointplot(x,y,color = 'red')\n",
    "ax.set_title('3blue1brown vs Pokemon')\n",
    "ax.set_xlabel('Keywords')\n",
    "ax.set_ylabel('Cos_Sim')\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = ax.get_figure()\n",
    "fig.savefig(\"plot2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create rolling average of keyword vectors to graph why it was a bad idea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#start trying to make rolling average\n",
    "pl=Playlist(\"https://www.youtube.com/watch?v=WcD6jjLMZso&list=PLUXSZMIiUfFS3P3IcWk95yTOZdmUEI7C4\")\n",
    "pl.populate_video_urls()\n",
    "url_list_nba=pl.video_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nba=make_corpus(url_list_nba)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue=get_topic_space(['https://www.youtube.com/watch?v=jsYwFizhncE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors=[]\n",
    "for i in range(len(poke)):\n",
    "    keys=keywords(poke[i],words=5,lemmatize='True', split='true')\n",
    "    try:\n",
    "        x=sum(list(map(model.word_vec,keys)))\n",
    "    except KeyError:\n",
    "        x=oov(keys)\n",
    "    vectors.append(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vectors)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get running average of vectos\n",
    "vec_c=[vectors[0]]\n",
    "vec_avg=(vectors[0]+vectors[1])/2\n",
    "vec_c.append(vec_avg)\n",
    "for i in range(len(vectors)):\n",
    "    if i>1:\n",
    "        vec_avg=(vec_avg+vectors[i])/2\n",
    "        vec_c.append(vec_avg)\n",
    "    else:\n",
    "        i=+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vec_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=[]\n",
    "for i in range(len(vec_c)):\n",
    "    results.append(cos_sim(vec_c[i],blue))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=results\n",
    "x=list(range(len(results)))\n",
    "sns.set(style=\"ticks\", rc={\"lines.linewidth\": 2})\n",
    "ax=sns.pointplot(x,y,color = 'red')\n",
    "ax.set_title('3blue1brown vs Pokemon')\n",
    "ax.set_xlabel('Keywords')\n",
    "ax.set_ylabel('Cos_Sim')\n",
    "#end of steps to make rollling average for graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript_better(url):\n",
    "    path=url\n",
    " \n",
    "    yt = YouTube(path)\n",
    " \n",
    "        \n",
    "    caption = yt.captions.get_by_language_code('en')\n",
    "    xml=caption.xml_captions\n",
    "    root = ET.fromstring(xml)\n",
    "    #gets the transcripts\n",
    "    doc=''\n",
    "    for child in root:\n",
    "        try:\n",
    "            doc=doc+\" \"+(child.text)\n",
    "        except TypeError:\n",
    "            pass\n",
    "    return doc.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
